{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ermias.tadesse\\10x\\Rossmann-Sales-Forecasting-ML\\log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "os.chdir(r'c:\\Users\\ermias.tadesse\\10x\\Rossmann-Sales-Forecasting-ML\\log')\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='store_sales.log', \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "print(os.getcwd())  # This prints the current working directory\n",
    "os.chdir(r'c:\\Users\\ermias.tadesse\\10x\\Rossmann-Sales-Forecasting-ML')  # Set the working directory to the project root\n",
    "from src.data_loader import DataLoader\n",
    "os.chdir(r'c:\\Users\\ermias.tadesse\\10x\\Rossmann-Sales-Forecasting-ML')  # Set the working directory to the project root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ermias.tadesse\\10x\\Rossmann-Sales-Forecasting-ML\\src\\data_loader.py:13: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.train = pd.read_csv(f\"{self.data_path}/train.csv\")\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(data_path='data')\n",
    "train_df, test_df, store_df, sample_submission_df = data_loader.load_data()\n",
    "logging.info(\"Loading train_data, test_data, store_data, and sample_submission_data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store\n",
      " \n",
      " Store                          0\n",
      "StoreType                      0\n",
      "Assortment                     0\n",
      "CompetitionDistance            3\n",
      "CompetitionOpenSinceMonth    354\n",
      "CompetitionOpenSinceYear     354\n",
      "Promo2                         0\n",
      "Promo2SinceWeek              544\n",
      "Promo2SinceYear              544\n",
      "PromoInterval                544\n",
      "dtype: int64 \n",
      " \n",
      " Train\n",
      " \n",
      " Store            0\n",
      "DayOfWeek        0\n",
      "Date             0\n",
      "Sales            0\n",
      "Customers        0\n",
      "Open             0\n",
      "Promo            0\n",
      "StateHoliday     0\n",
      "SchoolHoliday    0\n",
      "dtype: int64 \n",
      " \n",
      " Test\n",
      " \n",
      " Id                0\n",
      "Store             0\n",
      "DayOfWeek         0\n",
      "Date              0\n",
      "Open             11\n",
      "Promo             0\n",
      "StateHoliday      0\n",
      "SchoolHoliday     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_data_store = store_df.isnull().sum()\n",
    "missing_data_train = train_df.isnull().sum()\n",
    "missing_data_test = test_df.isnull().sum()\n",
    "logging.info(\"Show missing data.\")\n",
    "print('Store\\n \\n', missing_data_store , '\\n \\n Train\\n \\n', missing_data_train , '\\n \\n Test\\n \\n', missing_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Convert Date Columns: Convert the Date column to datetime format and extract useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
    "\n",
    "# Extract features\n",
    "train_df['Year'] = train_df['Date'].dt.year\n",
    "train_df['Month'] = train_df['Date'].dt.month\n",
    "train_df['Weekday'] = train_df['Date'].dt.weekday\n",
    "train_df['Day'] = train_df['Date'].dt.day\n",
    "\n",
    "logging.info(\"COnvert the data column to datetime format and extract useful features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Categorical Variables\n",
    "One-Hot Encoding: Use pd.get_dummies() for categorical variables like StoreType, Assortment, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df = pd.get_dummies(store_df, columns=['StoreType', 'Assortment'], drop_first=True)\n",
    "train_df = pd.get_dummies(train_df, columns=['StateHoliday'], drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=['StateHoliday'], drop_first=True)\n",
    "logging.info(\"wncode catagorical variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Datasets\n",
    "Combine Store Information with Train and Test Sets: Merge the store data with train and test data based on the Store column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(store_df, on='Store', how='left')\n",
    "test_df = test_df.merge(store_df, on='Store', how='left')\n",
    "logging.info(\"Merge datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle NaN Values After Merge\n",
    "Check for NaNs Again: After merging, check for any new NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()\n",
    "test_df.isnull().sum()\n",
    "logging.info(\"Handle NAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                           0\n",
      "Store                        0\n",
      "DayOfWeek                    0\n",
      "Date                         0\n",
      "Open                         0\n",
      "Promo                        0\n",
      "SchoolHoliday                0\n",
      "StateHoliday_a               0\n",
      "CompetitionDistance          0\n",
      "CompetitionOpenSinceMonth    0\n",
      "CompetitionOpenSinceYear     0\n",
      "Promo2                       0\n",
      "Promo2SinceWeek              0\n",
      "Promo2SinceYear              0\n",
      "PromoInterval                0\n",
      "StoreType_b                  0\n",
      "StoreType_c                  0\n",
      "StoreType_d                  0\n",
      "Assortment_b                 0\n",
      "Assortment_c                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values in the test DataFrame\n",
    "test_df['Open'] = test_df['Open'].fillna(0)\n",
    "test_df['CompetitionDistance'] = test_df['CompetitionDistance'].fillna(test_df['CompetitionDistance'].median())\n",
    "test_df['CompetitionOpenSinceMonth'] = test_df['CompetitionOpenSinceMonth'].fillna(0)\n",
    "test_df['CompetitionOpenSinceYear'] = test_df['CompetitionOpenSinceYear'].fillna(0)\n",
    "test_df['Promo2'] = test_df['Promo2'].fillna(0)\n",
    "test_df['Promo2SinceWeek'] = test_df['Promo2SinceWeek'].fillna(0)\n",
    "test_df['Promo2SinceYear'] = test_df['Promo2SinceYear'].fillna(0)\n",
    "test_df['PromoInterval'] = test_df['PromoInterval'].fillna('No Promo')\n",
    "\n",
    "# Verify if any missing values remain\n",
    "print(test_df.isnull().sum())\n",
    "logging.info(\"Filling missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Scaling Features\n",
    "Standardize Numeric Features: Use StandardScaler from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CompetitionDistance      Open     Promo  SchoolHoliday  StateHoliday_a\n",
      "0            -0.539198  0.452399  1.273237       2.144211       -0.142555\n",
      "1            -0.629927  0.452399  1.273237       2.144211       -0.142555\n",
      "2             1.127616  0.452399  1.273237       2.144211       -0.142555\n",
      "3            -0.623446  0.452399  1.273237       2.144211       -0.142555\n",
      "4             3.172897  0.452399  1.273237       2.144211       -0.142555\n",
      "   CompetitionDistance      Open     Promo  SchoolHoliday  StateHoliday_a\n",
      "0            -0.539198  0.452399  1.273237      -0.466372       -0.142555\n",
      "1             1.127616  0.452399  1.273237      -0.466372       -0.142555\n",
      "2             2.406888  0.452399  1.273237      -0.466372       -0.142555\n",
      "3             0.270879  0.452399  1.273237      -0.466372       -0.142555\n",
      "4            -0.440693  0.452399  1.273237      -0.466372       -0.142555\n",
      "(1017209, 29) (41088, 29)\n"
     ]
    }
   ],
   "source": [
    "# Define your numeric and categorical features\n",
    "numeric_features = ['CompetitionDistance', 'Open', 'Promo', 'SchoolHoliday', 'StateHoliday_a']\n",
    "categorical_features = ['StoreType_b', 'StoreType_c', 'StoreType_d', 'Assortment_b', 'Assortment_c', 'PromoInterval']\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numeric features in the training data\n",
    "train_df[numeric_features] = scaler.fit_transform(train_df[numeric_features])\n",
    "\n",
    "# Transform the numeric features in the test data\n",
    "test_df[numeric_features] = scaler.transform(test_df[numeric_features])\n",
    "\n",
    "# One-hot encode the categorical features\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_features, drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Ensure both train and test sets have the same columns after one-hot encoding\n",
    "train_df, test_df = train_df.align(test_df, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Verify the shapes\n",
    "# Verify the transformation\n",
    "print(train_df[numeric_features].head())\n",
    "print(test_df[numeric_features].head())\n",
    "print(train_df.shape, test_df.shape)\n",
    "logging.info(\"scaling features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "Separate Numeric and Categorical Features: This ensures you only scale numerical features.\n",
    "One-Hot Encoding: Converts categorical variables into a format suitable for machine learning.\n",
    "Align Train and Test DataFrames: After one-hot encoding, ensure that both DataFrames have the same columns. The align method fills any missing columns with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setting Up the Machine Learning Pipeline\n",
    "#### 1. Create a Pipeline\n",
    "Use Pipeline from sklearn to combine preprocessing and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define features and target\n",
    "X = train_df.drop(columns=['Sales', 'Date'])  # Drop target and other non-features\n",
    "y = train_df['Sales']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    "logging.info(\"Setting up machine learning pipline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train the Model\n",
    "Fit the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.fit(X_train, y_train)\n",
    "logging.info(\"Train Model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluate the Model\n",
    "Evaluate your model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 263.7933152446397\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred = model_pipeline.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "logging.info(\"Evaluate the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Serialize the Model\n",
    "Save your model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model_pipeline, 'data/sales_model.pkl')\n",
    "logging.info(\"Serialize the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build the Deep Learning Model (LSTM)\n",
    "#### 1. Prepare Data for LSTM\n",
    "Transform the data into a time series format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(0, 1, None), slice(None, -1, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ermias.tadesse\\10x\\Rossmann-Sales-Forecasting-ML\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (slice(0, 1, None), slice(None, -1, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m         y\u001b[38;5;241m.\u001b[39mappend(data[i \u001b[38;5;241m+\u001b[39m time_steps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Target variable\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n\u001b[1;32m---> 11\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Verify the shapes of the output\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, y shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m, in \u001b[0;36mcreate_time_series\u001b[1;34m(data, time_steps)\u001b[0m\n\u001b[0;32m      5\u001b[0m X, y \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m time_steps):\n\u001b[1;32m----> 7\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# All features except target\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(data[i \u001b[38;5;241m+\u001b[39m time_steps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Target variable\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "File \u001b[1;32mc:\\Users\\ermias.tadesse\\10x\\Rossmann-Sales-Forecasting-ML\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\ermias.tadesse\\10x\\Rossmann-Sales-Forecasting-ML\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3811\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[1;32m-> 3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m   3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(0, 1, None), slice(None, -1, None))"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Create a function to convert the data into time series\n",
    "# def create_time_series(data, time_steps=1):\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(data) - time_steps):\n",
    "#         X.append(data[i:(i + time_steps), :-1])  # All features except target\n",
    "#         y.append(data[i + time_steps, -1])  # Target variable\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# X, y = create_time_series(train_df[numeric_features])\n",
    "\n",
    "# # Verify the shapes of the output\n",
    "# print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "# # Assuming you have scaled data you can reshape it for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build the LSTM Model\n",
    "Use Keras or TensorFlow to create the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(50, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dense(1))  # Output layer\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Serving with REST API\n",
    "#### 1. Set Up Flask\n",
    "Create a simple Flask API to serve predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, request, jsonify\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# # Load the model\n",
    "# model = joblib.load('sales_model.pkl')\n",
    "\n",
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     data = request.json  # Expecting JSON input\n",
    "#     prediction = model.predict(pd.DataFrame(data))\n",
    "#     return jsonify({'prediction': prediction.tolist()})\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
